from bs4 import BeautifulSoup
import string
import os
import json


# In this method punctuations will be removed. All strings will be in lower case.
# Lastly if string contains any stopwords, they will be removed from the string
def normalize(raw_text, stopwords):
    punc_text = raw_text.translate(raw_text.maketrans("", "", string.punctuation))
    casefold_text = punc_text.casefold()
    norm_text = list()
    for word in casefold_text.split():
        if word not in stopwords:
            norm_text.append(word)

    return norm_text


def pre_processing():
    stopwords_file = open("stopwords.txt", 'r')
    stopwords = stopwords_file.read().splitlines()  # I have stored stopwords in this variable
    index_dict = {}  # I am keeping indexes of the words in here
    reuters = os.path.join(os.getcwd(), 'reuters21578')
    class_dict = {}
    for file in os.listdir(reuters):  # I am selecting files in reuters21578 directory
        if file.endswith(".sgm"):  # I am filtering .sgm files
            with open(os.path.join(reuters, file), "r", encoding="latin-1") as f:
                data = f.read()  # File read as requested encoding
                soup = BeautifulSoup(data, "html.parser")  # I used a third party library to ease .sgm file read
                news_list = soup.findAll("reuters")  # Stored all the news in a list
                for news in news_list:
                    isTopic = news.get("topics")  # Traversed all news and got their topics
                    topic_list = list()
                    if isTopic == "YES":
                        news_topic = ""
                        temp_topic_list = list()
                        news_topic = news.find("topics")
                        if "<d>" in str(news_topic):
                            # print(news_topic.find_all('d'))
                            temp_topic_list = news_topic.find_all('d')
                            for topic in temp_topic_list:
                                a = str(topic).replace("<d>", "")
                                b = a.replace("</d>", "")
                                topic_list.append(b)
                            # print(topic_list)
                            for top in topic_list:
                                if top in class_dict:
                                    class_dict[top] += 1
                                else:
                                    class_dict[top] = 1
    class_dict = sorted(class_dict.items(), key=lambda x: x[1], reverse=True)
    top10_topics = list()
    for k in class_dict[:10]:
        top10_topics.append(k[0])
    print(class_dict)
    print(top10_topics)

    TRAINING_SET = dict()
    TEST_SET = dict()
    vocabulary = dict()
    for el in top10_topics:
        TRAINING_SET[el] = {}
        TEST_SET[el] = {}

    print(TRAINING_SET)
    print(TEST_SET)

    for file in os.listdir(reuters):  # I am selecting files in reuters21578 directory
        if file.endswith(".sgm"):  # I am filtering .sgm files
            with open(os.path.join(reuters, file), "r", encoding="latin-1") as f:
                data = f.read()  # File read as requested encoding
                soup = BeautifulSoup(data, "html.parser")  # I used a third party library to ease .sgm file read
                news_list = soup.findAll("reuters")  # Stored all the news in a list
                for news in news_list:
                    isTopic = news.get("topics")  # Traversed all news and got their topics
                    isSplit = news.get("lewissplit")
                    # print(isSplit)
                    news_topic = ""
                    temp_topic_list = list()
                    topic_list = list()
                    if isTopic == "YES":
                        news_topic = news.find("topics")
                        if "<d>" in str(news_topic):
                            # print(news_topic.find_all('d'))
                            temp_topic_list = news_topic.find_all('d')
                            for topic in temp_topic_list:
                                a = str(topic).replace("<d>", "")
                                b = a.replace("</d>", "")
                                topic_list.append(b)
                            # print(topic_list)

                            for top in topic_list:
                                if top in top10_topics:
                                    news_id = news.get("newid")  # Traversed all news and got their ids
                                    news_title = news.find("title")  # Traversed all news and got their titles
                                    news_body = news.find("body")  # Traversed all news and got their bodys
                                    body = ""
                                    title = ""
                                    if news_title:  # If news has a title then take the title in string format
                                        title = news_title.text
                                    if news_body:  # If news has a body then take the body in string format
                                        body = news_body.text
                                    raw_text = title + " " + body  # Concatenate title and body for normalization process
                                    norm_text = normalize(raw_text, stopwords)  # Normalized text returned
                                    if isSplit == "TRAIN":
                                        if int(news_id) not in TRAINING_SET[top]:
                                            TRAINING_SET[top][int(news_id)] = dict()

                                        for word in norm_text:
                                            if word in vocabulary:
                                                vocabulary[word] += 1
                                            else:
                                                vocabulary[word] = 1
                                            if word in TRAINING_SET[top][int(news_id)]:
                                                TRAINING_SET[top][int(news_id)][word] += 1
                                            else:
                                                TRAINING_SET[top][int(news_id)][word] = 1
                                    else:
                                        if int(news_id) not in TEST_SET[top]:
                                            TEST_SET[top][int(news_id)] = dict()

                                        for word in norm_text:
                                            if word in vocabulary:
                                                vocabulary[word] += 1
                                            else:
                                                vocabulary[word] = 1
                                            if word in TEST_SET[top][int(news_id)]:
                                                TEST_SET[top][int(news_id)][word] += 1
                                            else:
                                                TEST_SET[top][int(news_id)][word] = 1

    with open("training_set.json", 'w', encoding="latin-1") as g:
        json.dump(TRAINING_SET, g, indent=4)
    with open("test_set.json", 'w', encoding="latin-1") as g:
        json.dump(TEST_SET, g, indent=4)
    with open("vocabulary.json", 'w', encoding="latin-1") as g:
        json.dump(vocabulary, g, indent=4)


pre_processing()
